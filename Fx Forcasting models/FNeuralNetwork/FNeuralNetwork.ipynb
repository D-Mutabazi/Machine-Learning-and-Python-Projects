{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import talib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "#specify device \n",
    "\n",
    "\n",
    "################################## Functions ####################################\n",
    "\n",
    "# Data loading\n",
    "def data_loader(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['Time'] = pd.to_datetime(data['Time'],format='%Y-%m-%d %H:%M:%S')\n",
    "    data.set_index('Time', inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Feature Engineering \n",
    "def multivariateFeatureEngineering(data):\n",
    "    \n",
    "    #Trend following Indicators:\n",
    "\n",
    "    #SMA - identofy long term trend\n",
    "    data['50_sma'] = data['Close'].rolling(window=50).mean() \n",
    "    data['200_sma'] = data['Close'].rolling(window=200).mean() \n",
    "\n",
    "    #EMA - trend analysis: more weight applied to recent points\n",
    "    data['50_ema'] = data['Close'].ewm(span=50, adjust=False).mean()\n",
    "    data['100_ema'] = data['Close'].ewm(span=100, adjust=False).mean()\n",
    "\n",
    "    #MACD\n",
    "    data['12_ema'] = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "    data['26_ema'] = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    data['MACD_line'] = data['12_ema']-data['26_ema'] # calculate the MACD line\n",
    "    data['Signal_line'] = data['MACD_line'].ewm(span=9, adjust=False).mean() # 9-preiod ema signal calculated from the Macdline\n",
    "    # data['MACD_histogram'] = data['MACD_line'] - data['Signal_line']\n",
    "\n",
    "    #ADX\n",
    "    # Calculate ADX using TA-Lib (14-period by default)\n",
    "    data['ADX'] = talib.ADX(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "\n",
    "    #Momentum indicators:\n",
    "\n",
    "    #RSI - 14-period\n",
    "    data['RSI'] = talib.RSI(data['Close'], timeperiod=14)\n",
    "    \n",
    "    #Stochastic Oscillator\n",
    "    data['stoch_k'], data['stoch_d'] = talib.STOCH(data['High'], data['Low'], data['Close'], fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "\n",
    "    #Volatility indicators#:\n",
    "\n",
    "    #ATR -Default period for ATR is 14\n",
    "    data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "\n",
    "    data = data.dropna() # drop rows that have NA\n",
    "\n",
    "    #drop certain featires\n",
    "    data = data.drop(columns=['12_ema', '26_ema'])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create Lag features and Multiple output response\n",
    "def multivariateFeatureLagMultiStep(data, n_past, future_steps, target_column):\n",
    "    features = []\n",
    "    response = []\n",
    "\n",
    "    max_future_step = max(future_steps)\n",
    "    num_features = data.shape[1]\n",
    "    group_feature_lags =  1 # change grouping of lagged features\n",
    "\n",
    "    # Adjust the loop to prevent index out of bounds\n",
    "    for i in range(n_past, len(data) - max_future_step + 1):\n",
    "\n",
    "        if group_feature_lags==1:\n",
    "                \n",
    "            lagged_features = []\n",
    "\n",
    "            for feature_idx in range(num_features):\n",
    "                feature_lags = data.iloc[i - n_past:i, feature_idx].values \n",
    "                lagged_features.extend(feature_lags) \n",
    "\n",
    "        elif group_feature_lags==0:\n",
    "            features.append(data.iloc[i - n_past:i, :].values)  # Take all columns as features\n",
    "\n",
    "\n",
    "        if group_feature_lags==1:\n",
    "            features.append(lagged_features)\n",
    "\n",
    "        # Extract the target values at specified future steps using .iloc\n",
    "        response.append([data.iloc[i + step - 1, target_column] for step in future_steps])\n",
    "\n",
    "    # Convert lists to NumPy arrays after the loop\n",
    "    features = np.array(features)  # Shape: (num_samples, n_past, num_features)\n",
    "    response = np.array(response)  # Shape: (num_samples, len(future_steps))\n",
    "\n",
    "    # Flatten the features to 2D array: (num_samples, n_past * num_features)\n",
    "    features_flat = features.reshape(features.shape[0], -1)\n",
    "\n",
    "    return features_flat, response\n",
    "\n",
    "\n",
    "# Unique feature combinations\n",
    "def featuresComblist(features):\n",
    "    import itertools\n",
    "\n",
    "    initial_feature = ['Close'] # Starting with the closing price\n",
    "\n",
    "    # Get all combinations of the features list and add to the initial feature (Closing Price)\n",
    "    feature_combinations = []\n",
    "    for i in range(len(features) + 1):\n",
    "        for combination in itertools.combinations(features, i):\n",
    "            feature_combinations.append(list(combination)+ initial_feature )\n",
    "    \n",
    "    return feature_combinations\n",
    "\n",
    "\n",
    "# Function to initialize CSV with headers\n",
    "def initialize_csv(file_name):\n",
    "    headers = ['lookback_window', 'features_used', 'learning_rate', 'number_of_hidden_layers', 'number_of_hidden_neurons',\n",
    "               'MSE_1_day', 'MAE_1_day', 'MAPE_1_day', 'MBE_1_day', 'RMSE_1_day', 'R2_1_day',\n",
    "               'MSE_3_day', 'MAE_3_day', 'MAPE_3_day', 'MBE_3_day', 'RMSE_3_day', 'R2_3_day',\n",
    "               'MSE_5_day', 'MAE_5_day', 'MAPE_5_day', 'MBE_5_day', 'RMSE_5_day', 'R2_5_day']\n",
    "    df = pd.DataFrame(columns=headers)\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Function to calculate performance metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Ensure y_true and y_pred are PyTorch tensors for MSE calculation\n",
    "    mse = F.mse_loss(torch.tensor(y_pred), torch.tensor(y_true)).item()\n",
    "    \n",
    "    # Convert to NumPy arrays for the remaining metrics\n",
    "    y_true = y_true.numpy() if isinstance(y_true, torch.Tensor) else y_true\n",
    "    y_pred = y_pred.numpy() if isinstance(y_pred, torch.Tensor) else y_pred\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # MAPE\n",
    "    mbe = np.mean(y_true - y_pred)  # Mean Bias Error (MBE)\n",
    "    rmse = np.sqrt(mse)  # RMSE\n",
    "    r2 = r2_score(y_true, y_pred)  # RÂ²\n",
    "    return mse, mae, mape, mbe, rmse, r2\n",
    "\n",
    "# Function to append results to CSV\n",
    "def append_to_csv(file_name, hyperparams, metrics):\n",
    "    row = hyperparams + metrics\n",
    "    df = pd.DataFrame([row])\n",
    "    df.to_csv(file_name, mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################### Model training ##########################################################\n",
    "\n",
    "\n",
    "class multipleOutputForexDataset:\n",
    "    def __init__(self, data, n_past, futureSteps, target_col):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        #creat design matix and response\n",
    "        self.features, self.labels = multivariateFeatureLagMultiStep(self.data, n_past, futureSteps, target_col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)  # Change to float32\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float32)  # Ensure both are float32\n",
    "        return features, labels\n",
    "\n",
    "\n",
    "######################################################## Data Loading ############################################################\n",
    "file_path='./Data/EURUSD_D1.csv'\n",
    "FXdata = data_loader(file_path)\n",
    "\n",
    "future_steps = [1 , 3, 5]\n",
    "target_col =  -1\n",
    "\n",
    "\n",
    "# ########################################## Neural Network model and Architecture development ##############################\n",
    "\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "features = ['Open', 'High', 'Low', 'Volume', '50_sma', '200_sma', '50_ema',\n",
    "       '100_ema', 'MACD_line', 'Signal_line', 'ADX', 'RSI', 'stoch_k',\n",
    "       'stoch_d', 'ATR']\n",
    "feature_combinations = featuresComblist(features)\n",
    "output_size = 3  # Output layer size (closing price t =  [1 , 3, 5])\n",
    "\n",
    "lookback_window_grid = [3, 5, 7, 10, 15]  \n",
    "hidden_neurons_grid = [8 ,10, 16, 32, 64, 128]  # Number of neurons per hidden layer\n",
    "hidden_layers_grid = [1, 2, 3, 4]  # Number of hidden layers\n",
    "learning_rate_grid = [0.001, 0.0001]  # Learning rates\n",
    "\n",
    "\n",
    "#Generate initial features\n",
    "multiVarData = multivariateFeatureEngineering(FXdata)\n",
    "col = [col for col in multiVarData.columns if col!='Close'] + ['Close'] #Arrange the columns\n",
    "multiVarData=   multiVarData[col]\n",
    "               \n",
    "\n",
    "# Initialize CSV file\n",
    "file_name = 'NN_hyperparameter_search_results.csv'\n",
    "initialize_csv(file_name)\n",
    "\n",
    "# Example of grid search loop for hyperparameters\n",
    "for features in feature_combinations:\n",
    "\n",
    "    feature_names_used = list(features)  # Store this for later use\n",
    "\n",
    "    data_subset = multiVarData[features]\n",
    "\n",
    "    for n_past in lookback_window_grid:\n",
    "\n",
    "        for hidden_size in hidden_neurons_grid:\n",
    "\n",
    "            for num_layers in hidden_layers_grid:\n",
    "\n",
    "                for learning_rate in learning_rate_grid:\n",
    "\n",
    "                    print(features)\n",
    "\n",
    "                    # Prepare the dataset for the current lookback window\n",
    "                    transformed_data = multipleOutputForexDataset(data_subset, n_past, future_steps, target_col)\n",
    "\n",
    "                    # Train-test split\n",
    "                    train_dataset, test_dataset = train_test_split(transformed_data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "                    # Load train and test data\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "                    # Initialize weights and biases for the layers\n",
    "                    W_hidden = []\n",
    "                    b_hidden = []\n",
    "                    input_size = data_subset.shape[1]  # number of original features\n",
    "\n",
    "                    # First layer initialization\n",
    "                    W_hidden.append(torch.randn(input_size * n_past, hidden_size, dtype=torch.float32, requires_grad=True))\n",
    "                    b_hidden.append(torch.randn(hidden_size, dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "                    # Initialize additional hidden layers if required\n",
    "                    for i in range(1, num_layers):\n",
    "                        W_hidden.append(torch.randn(hidden_size, hidden_size, dtype=torch.float32, requires_grad=True))\n",
    "                        b_hidden.append(torch.randn(hidden_size, dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "                    # Output layer initialization\n",
    "                    W_output = torch.randn(hidden_size, output_size, dtype=torch.float32, requires_grad=True)\n",
    "                    b_output = torch.randn(output_size, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "                    # Define forward propagation\n",
    "                    def forward(features):\n",
    "                        # Input to first hidden layer\n",
    "                        hidden_activation = F.relu(features @ W_hidden[0] + b_hidden[0])\n",
    "\n",
    "                        # Forward through remaining hidden layers\n",
    "                        for i in range(1, num_layers):\n",
    "                            hidden_activation = F.relu(hidden_activation @ W_hidden[i] + b_hidden[i])\n",
    "\n",
    "                        # Output layer\n",
    "                        output = hidden_activation @ W_output + b_output\n",
    "                        return output\n",
    "\n",
    "                    # Training the model\n",
    "                    num_epochs = 10\n",
    "\n",
    "                    for epoch in range(num_epochs):\n",
    "                        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "                            # Zero gradients\n",
    "                            for i in range(num_layers):\n",
    "                                W_hidden[i].grad = None\n",
    "                                b_hidden[i].grad = None\n",
    "                            W_output.grad = None\n",
    "                            b_output.grad = None\n",
    "\n",
    "                            # Forward pass\n",
    "                            predicted_close = forward(features)\n",
    "\n",
    "                            # Compute loss\n",
    "                            loss = F.mse_loss(predicted_close, labels)\n",
    "\n",
    "                            # Backward pass\n",
    "                            loss.backward()\n",
    "\n",
    "                            # Gradient descent\n",
    "                            with torch.no_grad():\n",
    "                                for i in range(num_layers):\n",
    "                                    W_hidden[i] -= learning_rate * W_hidden[i].grad\n",
    "                                    b_hidden[i] -= learning_rate * b_hidden[i].grad\n",
    "                                W_output -= learning_rate * W_output.grad\n",
    "                                b_output -= learning_rate * b_output.grad\n",
    "\n",
    "                \n",
    "\n",
    "                    # Evaluate on the test data and compute metrics for each forecast horizon\n",
    "                    mse_1, mae_1, mape_1, mbe_1, rmse_1, r2_1 = 0, 0, 0, 0, 0, 0\n",
    "                    mse_3, mae_3, mape_3, mbe_3, rmse_3, r2_3 = 0, 0, 0, 0, 0, 0\n",
    "                    mse_5, mae_5, mape_5, mbe_5, rmse_5, r2_5 = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for features, labels in test_loader:\n",
    "                            predicted_close = forward(features)\n",
    "\n",
    "                            # Compute metrics for each forecast horizon (1-day, 3-day, 5-day)\n",
    "                            for day_idx, day_name in zip(range(3), ['1_day', '3_day', '5_day']):\n",
    "                                y_true = labels[:, day_idx].numpy()\n",
    "                                y_pred = predicted_close[:, day_idx].numpy()\n",
    "\n",
    "                                mse, mae, mape, mbe, rmse, r2 = calculate_metrics(y_true, y_pred)\n",
    "\n",
    "                                # Assign to the correct metric based on the day\n",
    "                                if day_name == '1_day':\n",
    "                                    mse_1, mae_1, mape_1, mbe_1, rmse_1, r2_1 = mse, mae, mape, mbe, rmse, r2\n",
    "                                elif day_name == '3_day':\n",
    "                                    mse_3, mae_3, mape_3, mbe_3, rmse_3, r2_3 = mse, mae, mape, mbe, rmse, r2\n",
    "                                elif day_name == '5_day':\n",
    "                                    mse_5, mae_5, mape_5, mbe_5, rmse_5, r2_5 = mse, mae, mape, mbe, rmse, r2\n",
    "\n",
    "                    # Prepare hyperparameters\n",
    "                    hyperparams = [n_past, feature_names_used, learning_rate, num_layers, hidden_size]\n",
    "\n",
    "                    # Prepare the metrics for 1-day, 3-day, and 5-day forecasts\n",
    "                    metrics = [\n",
    "                        mse_1, mae_1, mape_1, mbe_1, rmse_1, r2_1,\n",
    "                        mse_3, mae_3, mape_3, mbe_3, rmse_3, r2_3,\n",
    "                        mse_5, mae_5, mape_5, mbe_5, rmse_5, r2_5\n",
    "                    ]\n",
    "\n",
    "                    # Append results to CSV\n",
    "                    append_to_csv(file_name, hyperparams, metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
