{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Close']\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n",
      "tensor([[1.0824, 1.0786, 1.0910],\n",
      "        [1.0786, 1.0910, 1.0910]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 318\u001b[0m\n\u001b[1;32m    315\u001b[0m y_true \u001b[38;5;241m=\u001b[39m labels[:, day_idx]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    316\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m predicted_close[:, day_idx]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 318\u001b[0m mse, mae, mape, mbe, rmse, r2 \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Assign to the correct metric based on the day\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m day_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1_day\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[60], line 142\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    139\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y_true, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m y_true\n\u001b[1;32m    140\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y_pred, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m y_pred\n\u001b[0;32m--> 142\u001b[0m mae \u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m mape \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m/\u001b[39m y_true)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# MAPE\u001b[39;00m\n\u001b[1;32m    144\u001b[0m mbe \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y_true \u001b[38;5;241m-\u001b[39m y_pred)  \u001b[38;5;66;03m# Mean Bias Error (MBE)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:207\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    144\u001b[0m     {\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m ):\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    0.85...\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    211\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(np\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:104\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    102\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    103\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 104\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    107\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1049\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import talib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "#specify device \n",
    "\n",
    "\n",
    "################################## Functions ####################################\n",
    "\n",
    "# Data loading\n",
    "def data_loader(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['Time'] = pd.to_datetime(data['Time'],format='%Y-%m-%d %H:%M:%S')\n",
    "    data.set_index('Time', inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Feature Engineering \n",
    "def multivariateFeatureEngineering(data):\n",
    "    \n",
    "    #Trend following Indicators:\n",
    "\n",
    "    #SMA - identofy long term trend\n",
    "    data['50_sma'] = data['Close'].rolling(window=50).mean() \n",
    "    data['200_sma'] = data['Close'].rolling(window=200).mean() \n",
    "\n",
    "    #EMA - trend analysis: more weight applied to recent points\n",
    "    data['50_ema'] = data['Close'].ewm(span=50, adjust=False).mean()\n",
    "    data['100_ema'] = data['Close'].ewm(span=100, adjust=False).mean()\n",
    "\n",
    "    #MACD\n",
    "    data['12_ema'] = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "    data['26_ema'] = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    data['MACD_line'] = data['12_ema']-data['26_ema'] # calculate the MACD line\n",
    "    data['Signal_line'] = data['MACD_line'].ewm(span=9, adjust=False).mean() # 9-preiod ema signal calculated from the Macdline\n",
    "    # data['MACD_histogram'] = data['MACD_line'] - data['Signal_line']\n",
    "\n",
    "    #ADX\n",
    "    # Calculate ADX using TA-Lib (14-period by default)\n",
    "    data['ADX'] = talib.ADX(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "\n",
    "    #Momentum indicators:\n",
    "\n",
    "    #RSI - 14-period\n",
    "    data['RSI'] = talib.RSI(data['Close'], timeperiod=14)\n",
    "    \n",
    "    #Stochastic Oscillator\n",
    "    data['stoch_k'], data['stoch_d'] = talib.STOCH(data['High'], data['Low'], data['Close'], fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "\n",
    "    #Volatility indicators#:\n",
    "\n",
    "    #ATR -Default period for ATR is 14\n",
    "    data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "\n",
    "    data = data.dropna() # drop rows that have NA\n",
    "\n",
    "    #drop certain featires\n",
    "    data = data.drop(columns=['12_ema', '26_ema'])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create Lag features and Multiple output response\n",
    "def multivariateFeatureLagMultiStep(data, n_past, future_steps, target_column):\n",
    "    features = []\n",
    "    response = []\n",
    "\n",
    "    max_future_step = max(future_steps)\n",
    "    num_features = data.shape[1]\n",
    "    group_feature_lags =  1 # change grouping of lagged features\n",
    "\n",
    "    # Adjust the loop to prevent index out of bounds\n",
    "    for i in range(n_past, len(data) - max_future_step + 1):\n",
    "\n",
    "        if group_feature_lags==1:\n",
    "                \n",
    "            lagged_features = []\n",
    "\n",
    "            for feature_idx in range(num_features):\n",
    "                feature_lags = data.iloc[i - n_past:i, feature_idx].values \n",
    "                lagged_features.extend(feature_lags) \n",
    "\n",
    "        elif group_feature_lags==0:\n",
    "            features.append(data.iloc[i - n_past:i, :].values)  # Take all columns as features\n",
    "\n",
    "\n",
    "        if group_feature_lags==1:\n",
    "            features.append(lagged_features)\n",
    "\n",
    "        # Extract the target values at specified future steps using .iloc\n",
    "        response.append([data.iloc[i + step - 1, target_column] for step in future_steps])\n",
    "\n",
    "    # Convert lists to NumPy arrays after the loop\n",
    "    features = np.array(features)  # Shape: (num_samples, n_past, num_features)\n",
    "    response = np.array(response)  # Shape: (num_samples, len(future_steps))\n",
    "\n",
    "    # Flatten the features to 2D array: (num_samples, n_past * num_features)\n",
    "    features_flat = features.reshape(features.shape[0], -1)\n",
    "\n",
    "    return features_flat, response\n",
    "\n",
    "\n",
    "# Unique feature combinations\n",
    "def featuresComblist(features):\n",
    "    import itertools\n",
    "\n",
    "    initial_feature = ['Close'] # Starting with the closing price\n",
    "\n",
    "    # Get all combinations of the features list and add to the initial feature (Closing Price)\n",
    "    feature_combinations = []\n",
    "    for i in range(len(features) + 1):\n",
    "        for combination in itertools.combinations(features, i):\n",
    "            feature_combinations.append(list(combination)+ initial_feature )\n",
    "    \n",
    "    return feature_combinations\n",
    "\n",
    "\n",
    "# Function to initialize CSV with headers\n",
    "def initialize_csv(file_name):\n",
    "    headers = ['lookback_window', 'features_used', 'learning_rate', 'number_of_hidden_layers', 'number_of_hidden_neurons',\n",
    "               'MSE_1_day', 'MAE_1_day', 'MAPE_1_day', 'MBE_1_day', 'RMSE_1_day', 'R2_1_day',\n",
    "               'MSE_3_day', 'MAE_3_day', 'MAPE_3_day', 'MBE_3_day', 'RMSE_3_day', 'R2_3_day',\n",
    "               'MSE_5_day', 'MAE_5_day', 'MAPE_5_day', 'MBE_5_day', 'RMSE_5_day', 'R2_5_day']\n",
    "    df = pd.DataFrame(columns=headers)\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Function to calculate performance metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Ensure y_true and y_pred are PyTorch tensors for MSE calculation\n",
    "    mse = F.mse_loss(torch.tensor(y_pred), torch.tensor(y_true)).item()\n",
    "    \n",
    "    # Convert to NumPy arrays for the remaining metrics\n",
    "    y_true = y_true.numpy() if isinstance(y_true, torch.Tensor) else y_true\n",
    "    y_pred = y_pred.numpy() if isinstance(y_pred, torch.Tensor) else y_pred\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # MAPE\n",
    "    mbe = np.mean(y_true - y_pred)  # Mean Bias Error (MBE)\n",
    "    rmse = np.sqrt(mse)  # RMSE\n",
    "    r2 = r2_score(y_true, y_pred)  # R²\n",
    "    return mse, mae, mape, mbe, rmse, r2\n",
    "\n",
    "# Function to append results to CSV\n",
    "def append_to_csv(file_name, hyperparams, metrics):\n",
    "    row = hyperparams + metrics\n",
    "    df = pd.DataFrame([row])\n",
    "    df.to_csv(file_name, mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################### Model training ##########################################################\n",
    "\n",
    "\n",
    "class multipleOutputForexDataset:\n",
    "    def __init__(self, data, n_past, futureSteps, target_col):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        #creat design matix and response\n",
    "        self.features, self.labels = multivariateFeatureLagMultiStep(self.data, n_past, futureSteps, target_col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)  # Change to float32\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float32)  # Ensure both are float32\n",
    "        return features, labels\n",
    "\n",
    "\n",
    "######################################################## Data Loading ############################################################\n",
    "file_path='./Data/EURUSD_D1.csv'\n",
    "FXdata = data_loader(file_path)\n",
    "\n",
    "future_steps = [1 , 3, 5]\n",
    "target_col =  -1\n",
    "\n",
    "\n",
    "# ########################################## Neural Network model and Architecture development ##############################\n",
    "\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "features = ['Open', 'High', 'Low', 'Volume', '50_sma', '200_sma', '50_ema',\n",
    "       '100_ema', 'MACD_line', 'Signal_line', 'ADX', 'RSI', 'stoch_k',\n",
    "       'stoch_d', 'ATR']\n",
    "feature_combinations = featuresComblist(features)\n",
    "output_size = 3  # Output layer size (closing price t =  [1 , 3, 5])\n",
    "\n",
    "lookback_window_grid = [3, 5, 7, 10, 15]  \n",
    "hidden_neurons_grid = [8 ,10, 16, 32, 64, 128]  # Number of neurons per hidden layer\n",
    "hidden_layers_grid = [1, 2, 3, 4]  # Number of hidden layers\n",
    "learning_rate_grid = [0.001, 0.0001]  # Learning rates\n",
    "\n",
    "\n",
    "#Generate initial features\n",
    "multiVarData = multivariateFeatureEngineering(FXdata)\n",
    "col = [col for col in multiVarData.columns if col!='Close'] + ['Close'] #Arrange the columns\n",
    "multiVarData=   multiVarData[col]\n",
    "               \n",
    "\n",
    "# Initialize CSV file\n",
    "file_name = 'NN_hyperparameter_search_results.csv'\n",
    "initialize_csv(file_name)\n",
    "\n",
    "# Example of grid search loop for hyperparameters\n",
    "for features in feature_combinations:\n",
    "\n",
    "    feature_names_used = list(features)  # Store this for later use\n",
    "\n",
    "    data_subset = multiVarData[features]\n",
    "\n",
    "    for n_past in lookback_window_grid:\n",
    "\n",
    "        for hidden_size in hidden_neurons_grid:\n",
    "\n",
    "            for num_layers in hidden_layers_grid:\n",
    "\n",
    "                for learning_rate in learning_rate_grid:\n",
    "\n",
    "                    print(features)\n",
    "\n",
    "                    # Prepare the dataset for the current lookback window\n",
    "                    transformed_data = multipleOutputForexDataset(data_subset, n_past, future_steps, target_col)\n",
    "\n",
    "                    # Train-test split\n",
    "                    train_dataset, test_dataset = train_test_split(transformed_data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "                    # Load train and test data\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "                    # Initialize weights and biases for the layers\n",
    "                    W_hidden = []\n",
    "                    b_hidden = []\n",
    "                    input_size = data_subset.shape[1]  # number of original features\n",
    "\n",
    "                    # First layer initialization\n",
    "                    W_hidden.append(torch.randn(input_size * n_past, hidden_size, dtype=torch.float32, requires_grad=True))\n",
    "                    b_hidden.append(torch.randn(hidden_size, dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "                    # Initialize additional hidden layers if required\n",
    "                    for i in range(1, num_layers):\n",
    "                        W_hidden.append(torch.randn(hidden_size, hidden_size, dtype=torch.float32, requires_grad=True))\n",
    "                        b_hidden.append(torch.randn(hidden_size, dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "                    # Output layer initialization\n",
    "                    W_output = torch.randn(hidden_size, output_size, dtype=torch.float32, requires_grad=True)\n",
    "                    b_output = torch.randn(output_size, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "                    # Define forward propagation\n",
    "                    def forward(features):\n",
    "                        # Input to first hidden layer\n",
    "                        hidden_activation = F.relu(features @ W_hidden[0] + b_hidden[0])\n",
    "\n",
    "                        # Forward through remaining hidden layers\n",
    "                        for i in range(1, num_layers):\n",
    "                            hidden_activation = F.relu(hidden_activation @ W_hidden[i] + b_hidden[i])\n",
    "\n",
    "                        # Output layer\n",
    "                        output = hidden_activation @ W_output + b_output\n",
    "                        return output\n",
    "\n",
    "                    # Training the model\n",
    "                    num_epochs = 10\n",
    "\n",
    "                    for epoch in range(num_epochs):\n",
    "                        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "                            # Zero gradients\n",
    "                            for i in range(num_layers):\n",
    "                                W_hidden[i].grad = None\n",
    "                                b_hidden[i].grad = None\n",
    "                            W_output.grad = None\n",
    "                            b_output.grad = None\n",
    "\n",
    "                            # Forward pass\n",
    "                            predicted_close = forward(features)\n",
    "\n",
    "                            # Compute loss\n",
    "                            loss = F.mse_loss(predicted_close, labels)\n",
    "\n",
    "                            # Backward pass\n",
    "                            loss.backward()\n",
    "\n",
    "                            # Gradient descent\n",
    "                            with torch.no_grad():\n",
    "                                for i in range(num_layers):\n",
    "                                    W_hidden[i] -= learning_rate * W_hidden[i].grad\n",
    "                                    b_hidden[i] -= learning_rate * b_hidden[i].grad\n",
    "                                W_output -= learning_rate * W_output.grad\n",
    "                                b_output -= learning_rate * b_output.grad\n",
    "\n",
    "                \n",
    "\n",
    "                    # Evaluate on the test data and compute metrics for each forecast horizon\n",
    "                    mse_1, mae_1, mape_1, mbe_1, rmse_1, r2_1 = 0, 0, 0, 0, 0, 0\n",
    "                    mse_3, mae_3, mape_3, mbe_3, rmse_3, r2_3 = 0, 0, 0, 0, 0, 0\n",
    "                    mse_5, mae_5, mape_5, mbe_5, rmse_5, r2_5 = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for features, labels in test_loader:\n",
    "                            predicted_close = forward(features)\n",
    "\n",
    "                            # Compute metrics for each forecast horizon (1-day, 3-day, 5-day)\n",
    "                            for day_idx, day_name in zip(range(3), ['1_day', '3_day', '5_day']):\n",
    "                                y_true = labels[:, day_idx].numpy()\n",
    "                                y_pred = predicted_close[:, day_idx].numpy()\n",
    "\n",
    "                                mse, mae, mape, mbe, rmse, r2 = calculate_metrics(y_true, y_pred)\n",
    "\n",
    "                                # Assign to the correct metric based on the day\n",
    "                                if day_name == '1_day':\n",
    "                                    mse_1, mae_1, mape_1, mbe_1, rmse_1, r2_1 = mse, mae, mape, mbe, rmse, r2\n",
    "                                elif day_name == '3_day':\n",
    "                                    mse_3, mae_3, mape_3, mbe_3, rmse_3, r2_3 = mse, mae, mape, mbe, rmse, r2\n",
    "                                elif day_name == '5_day':\n",
    "                                    mse_5, mae_5, mape_5, mbe_5, rmse_5, r2_5 = mse, mae, mape, mbe, rmse, r2\n",
    "\n",
    "                    # Prepare hyperparameters\n",
    "                    hyperparams = [n_past, feature_names_used, learning_rate, num_layers, hidden_size]\n",
    "\n",
    "                    # Prepare the metrics for 1-day, 3-day, and 5-day forecasts\n",
    "                    metrics = [\n",
    "                        mse_1, mae_1, mape_1, mbe_1, rmse_1, r2_1,\n",
    "                        mse_3, mae_3, mape_3, mbe_3, rmse_3, r2_3,\n",
    "                        mse_5, mae_5, mape_5, mbe_5, rmse_5, r2_5\n",
    "                    ]\n",
    "\n",
    "                    # Append results to CSV\n",
    "                    append_to_csv(file_name, hyperparams, metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
